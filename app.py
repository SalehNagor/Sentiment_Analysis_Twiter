import streamlit as st
import torch
import numpy as np
from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast
import os

# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(page_title="Sentiment Analysis Dashboard", page_icon="ğŸ“Š")

st.title("ğŸ“Š Sentiment Analysis System")
st.write("This dashboard uses a **Pruned DistilBERT** model to classify customer feedback.")

# 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ (ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡)
@st.cache_resource
def load_model():
    # Ù†Ø­Ø¯Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±: Ù‡Ù„ Ù†Ø­Ù† Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¯ÙˆÙƒØ± Ø£Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ø­Ù„ÙŠØŸ
    # ÙÙŠ Ø§Ù„Ø¯ÙˆÙƒØ± ÙˆØ¶Ø¹Ù†Ø§Ù‡ ÙÙŠ final_modelØŒ Ù…Ø­Ù„ÙŠØ§Ù‹ Ù‡Ùˆ ÙÙŠ models/pruned_model
    if os.path.exists("./final_model"):
        model_path = "./final_model"
    else:
        # Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ù„ÙŠ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø®ÙÙ ÙƒÙ…Ø§ Ø­Ø¯Ø¯Ù†Ø§Ù‡ ÙÙŠ main.py
        model_path = "./models/pruned_model" 
        
        # Ù…Ù„Ø§Ø­Ø¸Ø©: Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¬Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø§ÙŠØ²Ø± ÙÙŠ Ù…Ø¬Ù„Ø¯ prunedØŒ Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ù„ØªÙˆØ¬ÙŠÙ‡Ù‡ Ù„Ù…Ø¬Ù„Ø¯ distilbert_finetuned
        # Ù„ÙƒÙ† Ø§Ù„Ø¯ÙˆÙƒØ± Ø³ÙŠØ¬Ù…Ø¹Ù‡Ù…ØŒ Ù…Ø­Ù„ÙŠØ§Ù‹ ØªØ£ÙƒØ¯ Ø£Ù† Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙˆÙƒÙ†Ø§ÙŠØ²Ø± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø¬Ø§Ù†Ø¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
        if not os.path.exists(os.path.join(model_path, "vocab.txt")):
             # ÙÙŠ Ø­Ø§Ù„ ÙƒÙ†Øª ØªØ´ØºÙ„ Ù…Ø­Ù„ÙŠØ§Ù‹ ÙˆØ§Ù„ØªÙˆÙƒÙ†Ø§ÙŠØ²Ø± ÙÙŠ Ù…ÙƒØ§Ù† Ø¢Ø®Ø±
             tokenizer_path = "./models/distilbert_finetuned"
             return (DistilBertTokenizerFast.from_pretrained(tokenizer_path),
                     DistilBertForSequenceClassification.from_pretrained(model_path))

    tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)
    model = DistilBertForSequenceClassification.from_pretrained(model_path)
    return tokenizer, model

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
try:
    tokenizer, model = load_model()
    st.success("Model loaded successfully! âœ…")
except Exception as e:
    st.error(f"Error loading model: {e}")
    st.stop()

# 3. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù„Ø¥Ø¯Ø®Ø§Ù„
text_input = st.text_area("Enter Customer Feedback:", height=100)

if st.button("Analyze Sentiment"):
    if text_input.strip() == "":
        st.warning("Please enter some text first.")
    else:
        with st.spinner("Analyzing..."):
            # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Tokenization)
            inputs = tokenizer(text_input, return_tensors="pt", truncation=True, padding=True, max_length=128)
            
            # Ø§Ù„ØªÙˆÙ‚Ø¹ (Prediction)
            with torch.no_grad():
                outputs = model(**inputs)
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©
            logits = outputs.logits
            probs = torch.nn.functional.softmax(logits, dim=-1)
            predicted_class = torch.argmax(probs, dim=-1).item()
            confidence = probs[0][predicted_class].item()

            # 4. ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¥Ù„Ù‰ Ù†ØµÙˆØµ (Ø­Ø³Ø¨ ØªØ¯Ø±ÙŠØ¨Ùƒ: 0ØŒ 1ØŒ 2)
            # ØªØ£ÙƒØ¯ Ù…Ù† ØªØ±ØªÙŠØ¨ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª Ù„Ø¯ÙŠÙƒØŒ ØºØ§Ù„Ø¨Ø§Ù‹ ØªÙƒÙˆÙ† ÙƒØ§Ù„ØªØ§Ù„ÙŠ:
            labels_map = {0: "Negative ğŸ˜ ", 1: "Neutral ğŸ˜", 2: "Positive ğŸ˜ƒ"}
            sentiment = labels_map.get(predicted_class, "Unknown")

            # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            st.markdown("### Result:")
            if predicted_class == 2:
                st.success(f"**Sentiment:** {sentiment} (Confidence: {confidence:.2%})")
            elif predicted_class == 0:
                st.error(f"**Sentiment:** {sentiment} (Confidence: {confidence:.2%})")
            else:
                st.info(f"**Sentiment:** {sentiment} (Confidence: {confidence:.2%})")

st.write("Made by:")
st.write("Saleh Nagor / Majed Alfahmi / Anas Almuwalled / Rayan Aloufi")

st.write("Supervised by:")
st.write("Dr.Mohammed Arif")
